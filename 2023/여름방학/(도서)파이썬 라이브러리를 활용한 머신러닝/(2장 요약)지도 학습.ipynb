{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2dba771-ad98-4b5c-bd19-6989e3f06fd5",
   "metadata": {},
   "source": [
    "# **<span style=\"color:blue\">분류와 회귀</span>** \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec68b38-d657-4503-b467-e54aaef74f97",
   "metadata": {},
   "source": [
    "- 분류: 미리 정의된, 가능성 있는 여러 클래스 레이블 중 하나를 예측하는 것\n",
    "   - 이진 분류: 두개의 클래스로 분류하는 것 -> 양성 클래스, 음성 클래스로 분류함\n",
    "   - 다중 분류: 셋 이상의 클래스로 분류하는 것 \n",
    "- 회귀: 연속적인 숫자, 실수를 예측하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba611e29-974f-4fed-8e4e-ddaa5a5cea78",
   "metadata": {},
   "source": [
    "# **<span style=\"color:blue\">일반화, 과대적합, 과소적합</span>** \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5863837-04c1-42cf-a72b-f2084af16118",
   "metadata": {},
   "source": [
    "- 일반화: 모델이 처음 보는 데이터에 대해 정확하게 예측할 수 있는 것\n",
    "- 과대적합: 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것, 테스트 세트에서 잘 맞지 않음\n",
    "   - 모델을 복잡하게 할수록 훈련 데이터에 대해서는 정확히 예측할 수 있지만, 일반화되기 힘듬\n",
    "- 과소적합: 모델이 너무 간단해서 훈련 세트에 잘 맞지 않는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1053a24-4c3f-48d8-8b7e-41bc54fee277",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">모델 복잡도와 데이터셋 크기의 관계</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c9b7a-41d5-4c52-b830-63ad8a2f8610",
   "metadata": {},
   "source": [
    "- 데이터셋에 다양한 데이터 포인트나 많을수록 과대적합 없이 더 복잡한 모델을 만들 수 있음 -> 다양성을 키워주기 때문\n",
    "- 같은 데이터 포인트를 중복하거나 매우 비슷한 데이터를 모으는 것은 도움이 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670035fe-9db0-49bc-944e-05d7be2f1196",
   "metadata": {},
   "source": [
    "# **<span style=\"color:blue\">지도 학습 알고리즘</span>** \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8ea41-f765-4778-ad70-9999708821f9",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">k-최근접 이웃</span>** \n",
    "### **k-최근접 이웃 분류**\n",
    "- 가장 가까운 훈련 데이터 포인트 하나를 최근접 이웃으로 찾아 예측에 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ad3b3-266e-4f21-abc0-ffd91442ad58",
   "metadata": {},
   "source": [
    "### **KNeighborsClassifier 분석**\n",
    "- 2차원 데이터셋을 xy평면에 그려볼 수 있음\n",
    "- 각 데이터 포인트가 속한 클래스에 따라 평면에 색을 칠함 -> 클래스 0과 클래스 1로 지정한 영역으로 나누는 결정 경계를 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd5eaa-1daa-424f-9196-e6bfe3a9a83d",
   "metadata": {},
   "source": [
    "### **k-최근접 이웃 회귀**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f21e5-e3ba-43fb-b383-be1ee2e808f7",
   "metadata": {},
   "source": [
    "### **KNeighborsRegressor 분석**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cedf3-febd-4116-9837-8caca6267cb6",
   "metadata": {},
   "source": [
    "### **장단점과 매개변수**\n",
    "- knn의 중요한 매개변수는 데이터 포인트 사이의 거리를 재는 방법(metric 매개변수)과 이웃의 수임\n",
    "- 기본적으로 metric=2(유클리디안 거리)임\n",
    "- 데이터를 전처리하는 과정이 중요함. 같은 스케일을 갖도록 정규화하는 것이 일반적임\n",
    "- 특성이 많은 데이터셋과 특성 값 대부분이 0인 데이터셋에 잘 작동하지 않음\n",
    "- 훈련 세트의 특성의 수나 샘플의 수가 매우 크면 예측이 느려짐\n",
    "   - algorithm 매개변수는 kd_tree, ball_tree, brute가 있음. 기본값은 \"auto\"임\n",
    "   - 특성의 개수가 15개 이상이거나 이웃의 크기가 샘플 개수의 절반 이상일 때 자동으로 brute를 사용함\n",
    "   - '_fit_method' 속성을 확인하면 어떤 알고리즘이 선택되었는지 알 수 있음\n",
    "   - n_jobs=-1을 사용할 수 있음\n",
    "- knn이 이해하기는 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해서 현업에서 잘 쓰이지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f36197-b81d-468f-83e2-cbd385575111",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">선형 모델</span>** \n",
    "- 입력 특성에 대한 선형 함수를 만들어 예측함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be943394-8349-4dec-9da7-1b4e37cb8c41",
   "metadata": {},
   "source": [
    "### **회귀의 선형 모델**\n",
    "- 예측값은 입력 특성에 w의 각 가중치를 곱해서 더한 가중치 합으로 볼 수 있음 (단, 절편이 있음)\n",
    "- w 가중치는 모델이 학습하는 파라미터임 (모델 파라미터, 파라미터, 계수라고 부름) <-> 하이퍼 파라미터는 사람이 직접 설정하는 파라미터임\n",
    "- 특성이 적은 데이터(아래와 같은 1차원 데이터)에서는 예측에 제약이 많을 수 있으나, 특성이 많은 경우 훌륭한 성능을 낼 수 있음\n",
    "- 훈련 데이터보다 특성이 더 많은 경우(불충분한 시스템, underdetermined system 이라고 부름)에서도  모델링할 수 있음\n",
    "- 모델 파라미터 w와 b(절편)은 학습하는 방법과 모델의 복잡도를 제어하는 방법에서 차이가 남"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb21f5-9e89-46fa-bf86-35d39cedf8d6",
   "metadata": {},
   "source": [
    "### **선형 회귀(최소 제곱법)**\n",
    "- 가장 간단하고 오래된 회귀용 선형 알고리즘\n",
    "- 예측과 훈련 세트에 있는 타깃 y 사이의 평균제곱오차를 최소화하는 파라미터 w,b를 찾음\n",
    "- 평균제곱오차(MSE): 예측값과 타깃 값의 차이를 제곱하여 더한 후에 샘플의 개수를 나눈 것\n",
    "- 매개변수, 모델의 복잡도를 제어할 방법이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f886d-53d1-4ce4-947b-aa74f3dbbecf",
   "metadata": {},
   "source": [
    "### **릿지 회귀**\n",
    "- 가중치의 절댒값을 가능한 작게 만듬\n",
    "- 모든 특성이 출력에 주는 영향을 최소한으로 만듬 -> 규제\n",
    "- 규제: 과대적합이 되지 않도록 모델을 강제로 제한하는 것 -> 릿지 회귀에서는 L2 규제를 사용함\n",
    "- alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지를 지정할 수 있음\n",
    "- alpha=1.0이 기본값임, alpha가 커질수록 더 일반화됨\n",
    "- alpha가 작아질수록 LinearRegression과 비슷해짐\n",
    "- solver 매개변수로 여러가지 알고리즘을 지정할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec523e39-6cef-4128-b53f-c6181722831e",
   "metadata": {},
   "source": [
    "### **라쏘 회귀**\n",
    "- L1 규제를 사용함 -> 어떤 계수는 정말 0이 되기도 함\n",
    "- 특성 선택이 자동으로 이뤄진다고 볼 수 있음\n",
    "- 가장 중요한 특성이 무엇인지 드러내줌\n",
    "- 과소적합을 줄이기 위해 alpha값을 줄이면, max_iter(반복 실행하는 기본값)을 늘려야 함\n",
    "- alpha를 낮출수록 LinearRegression과 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24710f78-f614-4eab-a1ca-525de10a7e6e",
   "metadata": {},
   "source": [
    "- 보통은 릿지 회귀를 선호함\n",
    "- 특성이 많고 그 중 일부분만 중요하다면 Lasso가 좋을 수도 있음\n",
    "- 입력 특성 중 일부만 사용하므로 쉽게 해석할 수 있는 모델을 만들어줌\n",
    "- Lasso와 Ridge의 규제를 결합한 ElasticNet을 제공하지만, L1 규제와 L2규제를 위한 매개변수 두 개를 조정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad7dcf-c3ca-470d-b54e-49b6779f3f5c",
   "metadata": {},
   "source": [
    "### **분류용 선형 모델**\n",
    "- 이진 분류: 가중치 합을 임계치 0과 비교함. 0보다 작으면 음성 클래스, 0보다 크면 양성 클래스로 분류함(시그모이드는 0.5를 기준으로 함)\n",
    "- 분류형 선형 모델에서는 결정 경계가 입력의 선형 함수임\n",
    "- 선형 모델을 학습시키는 알고리즘은 두가지로 구분할 수 있음\n",
    "   - 특정 계수와 절편의 조합이 훈련 데이터에 얼마나 잘 맞는지 측정하는 방법\n",
    "   - 사용할 수 있는 규제가 있는지, 있다면 어떤 방식인지\n",
    "- 가장 널리 알려진 두 개의 선형 분류 알고리즘은 로지스틱 회귀와 서포트 벡터 머신임\n",
    "- 기본적으로 L2 규제를 사용함\n",
    "- C의 값이 높아지면 규제가 감소함 -> C의 값이 낮아지면 데이터 포인트 중 다수에 맞추려고 함\n",
    "- C의 기본값은 1.0임\n",
    "- 저차원에서는 회귀와 비슷하게 매우 제한적이지만, 고차원에서는 선형 모델이 매우 강력해짐 -> 과대적합을 조심해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87538f3d-d2cb-4c2a-90fd-8fccb048e7ea",
   "metadata": {},
   "source": [
    "- LinearSVC는 loss 매개변수에 손실 함수를 지정함. 기본값은 squared_hinge임 -> L1, L2 둘다 사용가능\n",
    "- loss='hinge'-> L2만 사용 가능\n",
    "- LogisticRegression은 L1, L2, elasticnet(L1과 L2를 모두 사용하는 규제), none을 지정할 수 있음. 이것도 손실 함수에 따라 사용할 수 있는 규제가 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35283714-9906-4c50-b009-063bbcccfaec",
   "metadata": {},
   "source": [
    "### **다중 클래스 분류용 선형 모델**\n",
    "- 많은 선형 분류 모델은 이진 분류만을 지원함(로지스틱 회귀 제외)\n",
    "- 이진 분류 알고리즘을 다중 클래스 분류 알고리즘으로 확장하는 보편적인 방법은 일대다 방법임 \n",
    "- 일대다 기법: 각 클래스를 다른 모든 클래스와 구분하도록 이진 분류 모델을 학습시킴 -> 클래스의 수만큼 이진 분류 모델이 만들어짐, 가장 높은 점수를 내는 클래스를 예측값으로 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a6a54-8396-44fe-bf28-f7a421312a08",
   "metadata": {},
   "source": [
    "### **장단점과 매개변수**\n",
    "- alpha와 C 매개변수를 조정하는 일이 매우 중요함 -> 보통 로그 스케일(0.01,0.1,1,10)로 최적치를 정함\n",
    "- L1, L2 규제를 정해야 함 -> 중요한 특성이 많지 않다고 생각하면 L1 규제를 사용함. 그렇지 않으면 기본적으로 L2를 사용함\n",
    "- 학습 속도와 예측이 빠름. 매우 큰 데이터셋과 희소한 데이터셋에서도 잘 작동함\n",
    "- 대용량 데이터셋이라면 solver=\"sag\" or \"saga\"를 주거나, SGDClassifier나 SGDRegressor을 사용할 수 있음\n",
    "- 예측이 어떻게 만들어지는지 비교적 쉽게 이해할 수 있음. 다만 특성들이 서로 깊게 연관되어 있을 때는 이해하기 어려움\n",
    "- 샘플에 비해 특성이 많을 때 잘 작동함\n",
    "- 저차원의 데이터셋에는 성능이 좋지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751eaf08-f593-47b7-aeee-f18b62fae322",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">나이브 베이즈 분류기</span>** \n",
    "- 선형 모델과 매우 유사함\n",
    "- LogisticRegression이나 LinearSVC 같은 선형 분류기보다 훈련 속도가 빠르지만, 일반화 성능이 조금 뒤떨어짐\n",
    "- 각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합함\n",
    "   - GaussianNB: 연속적인 어떤 데이터에도 적용할 수 있음, 클래스별로 각 특성의 분산과 평균을 저장함\n",
    "   - BernoulliNB: 이진 데이터에 적용함 -> 텍스트 데이터를 분류할 때 사용, 각 클래스의 특성 중 0이 아닌 것이 몇개인지 셈\n",
    "   - MultinomialNB: 카운트 데이터에 적용함 -> 텍스트 데이터를 분류할 때 사용, 클래스별 특성의 평균 계산\n",
    "- 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를 예측값으로 함\n",
    "- BernoulliNB와 MultinomialNB의 예측 공식은 선형 모델과 형태가 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f059218-1afc-4bc1-843b-0a5ce0fb3708",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **장단점과 매개변수**\n",
    "- BernoulliNB와 MultinomialNB은 모델의 복잡도를 조절하는 alpha 매개변수를 갖고 있음(선형 모델과는 다른 의미임). alpha에 따른 알고리즘 성능 변동은 비교적 크지 않음\n",
    "- GaussianNB는 대부분 매우 고차원인 데이터셋에 사용하고 다른 두 모델은 텍스트 같은 희소한 데이터를 카운트하는 데 사용함\n",
    "- 선형 모델의 장단점과 비슷함\n",
    "- 훈련과 예측 속도가 빠르며 훈련 과정을 이해하기 쉬움\n",
    "- 희소한 고차원 데이터에서 잘 작동하며 비교적 매개변수에 민감하지 않음\n",
    "- 선형 모델로도 학습 시간이 너무 오래 걸릴 때, 나이브 베이즈 모델을 시도해볼만 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb6160-66c2-4928-9631-b329e664aec3",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">결정 트리</span>** \n",
    "- 분류와 회귀 문제에서 많이 사용하는 모델\n",
    "- 결정을 하기 위해 예/아니오 질문을 이어나가며 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211a948-d2b4-47f0-a605-3ec26dc8ab6c",
   "metadata": {},
   "source": [
    "### **결정 트리 만들기**\n",
    "- 가능한 모든 테스트에서 타깃 값에 대해 가장 많은 정보를 가진 것을 고름\n",
    "- 루트 노드: 맨 위의 노드. 전체 데이터셋을 나타냄\n",
    "- 각 테스트는 하나의 축을 따라 데이터를 둘로 나누는 것으로 생각할 수 있음 -> 계층적으로 영역을 분할해가는 알고리즘\n",
    "- 데이터를 분할하는 것은 리프가 한 개의 타깃 값(하나의 클래스 혹은 회귀 분석 결과)을 가질 때까지 반복됨\n",
    "- 순수 노드: 타깃 하나로만 이뤄진 리프 노드\n",
    "- 회귀 문제에서는 태스트 결과에 따라 새로운 데이터 포인트에 해당되는 리프 노드를 찾음 -> 리프 노드의 훈련 데이터 평균값을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a851e39-553b-440a-b7af-ebfdebb19dfd",
   "metadata": {},
   "source": [
    "### **결정 트리의 복잡도 제어하기**\n",
    "- 모든 리프 노드가 순수 노드가 될 때까지 진행하면 모델이 매우 복잡해지고 과대적합됨 -> 결정 경계가 이상치에 너무 민감해짐\n",
    "- 과대적합을 막는 방법은 두 가지임\n",
    "   - 트리 생성을 일찍 중단하기(사전 가지치기)\n",
    "      - 트리의 최대 깊이 제한, 리프의 최대 개수 제한, 노드가 분할하기 위한 포인트의 최소 개수 지정\n",
    "      - 사이킷런에는 사전 가지치기만 지원했지만, 비용 복잡도 기반의 사후 가지치기를 위한 ccp_alpha 매개변수가 추가됨\n",
    "   - 트리를 만든 후 데이터 포인트가 적은 노드를 삭제하거나 병합하기(사후 가지치기, 가지치기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea4ed3-9cdb-4882-89b7-191a6af431ca",
   "metadata": {},
   "source": [
    "### **결정 트리 분석**\n",
    "- export_graphviz 함수를 이용해 트리를 시각화할 수 있음 -> 저장용 텍스트 파일인 .dot 파일을 만듬\n",
    "- 굳이 .dot 파일을 안 만들고 plot_tree()로 트리를 그릴 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e7d6a-4371-482d-9531-77cb73976a05",
   "metadata": {},
   "source": [
    "### **트리의 특성 중요도**\n",
    "- 특성 중요도: 트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 중요도\n",
    "- 0은 전혀 사용되지 않은 것이고, 1은 완벽하게 타깃 클래스를 예측했다는 뜻임\n",
    "- 특성 즁요도의 전체 합은 1임\n",
    "- feature_importance_값이 낮다고 해서 이 특성이 유용하지 않은 것은 아님. 단지 그 트리가 그 특성을 선택하지 않았고, 다른 특성이 동일한 정보를 지니고 있을 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5fd96-fd37-4b0a-8da4-a179b1e08360",
   "metadata": {},
   "source": [
    "- DecisionTreeRegressor과 다른 모든 트리 기반 회귀 모델은 외삽, 즉 훈련 데이터의 범위 밖의 포인트에 대해 예측을 할 수 없음\n",
    "- 모델이 가진 데이터 범위 밖으로 나가면 단순히 마지막 포인트를 이용해 예측하는 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa48d75-1c24-4fb4-9f10-e67a0a0b3eaf",
   "metadata": {},
   "source": [
    "### **장단점과 매개변수**\n",
    "- 결정 트리에서 모델 복잡도를 조절하기 위해서는 사전치기 매개변수를 바꾸면 됨\n",
    "- 보통은 사전 가지치기 방법 중 max_depth, max_leaf_nodes,min_samples_leaf 중 하나만 지정해도 과대적합을 막는 데에 충분함\n",
    "- 만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉬움(단, 비교적 작은 트리일 때 이해할 수 있음)\n",
    "- 데이터 스케일에 영향을 받지 않음 -> 표준화나 정규화를 할 필요가 없음\n",
    "- 그러나, 사전 가지치기를 하더라도 과대적합되는 경향이 있기 때문에 일반화 성능이 좋지 않음 -> 앙상블이 주로 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b439de-406e-44a6-afbc-e3a748502043",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">결정 트리의 앙상블</span>** \n",
    "- 앙상블: 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법\n",
    "- 랜덤 포레스트와 그레이디언트 부스팅 모델이 분류와 회귀 문제의 다양한 데이터셋에서 효과적이라고 입증됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e655a-9e92-4671-a5a0-309e52cd20d4",
   "metadata": {},
   "source": [
    "### **랜덤 포레스트**\n",
    "- 결정 트리가 훈련 데이터에 과대적합되는 경향이 있다는 단점을 피할 수 있는 방법\n",
    "- 조금씩 다른 여러 결정 트리의 묶음\n",
    "- 서로 다른 방향으로 과대적합된 트리를 많이 만들면 그 결과를 평균냄으로써 과대적합된 양을 줄일 수 있음 + 트리 모델의 예측 성능 유지\n",
    "- 결정 트리를 많이 만드렁야 하고, 각각의 트리는 타깃 예측을 잘 하며 다른 트리와 구별되어야 함\n",
    "- 트리들이 달라지도록 트리 생성 시 무작위성을 주입함 -> 2가지 방법이 있음\n",
    "   - 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하기\n",
    "   - 분할 테스트에서 특성을 무작위로 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbf9b0-0376-4a3a-9cbe-30274fb1059e",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:purple\">랜덤 포레스트 구축</span>** \n",
    "- 생성할 트리의 개수를 정해야 함(n_estimators 매개변수)\n",
    "- 데이터의 부트스트랩 샘플을 생성함\n",
    "   - n_samples개의 데이터 포인트 중에서 무작위로 데이터를 n_samples 횟수만큼 반복 추출함\n",
    "   - max_samples 매개변수(0~1의 값)를 사용해서 훈련 데이터 샘플 개수만큼 부트스트랩을 안해도 되게 할 수 있음. 기본값은 None임\n",
    "- 각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는 것이 아니라, 알고리즘이 각 노드에서 후보 특성을 무작위로 선택한 후, 이 후보들 중에서 최선의 테스트를 찾음\n",
    "   - 몇 개의 특성을 고를지는 max_features 매개변수로 조정할 수 있음 -> 핵심 매개변수임. 값을 크게 하면 랜덤 포레스트들의 트리가 비슷해짐\n",
    "   - 후보 특성을 고르는 것은 노드마다 반복됨\n",
    "- 분류를 할 때는 모든 트리의 예측을 만들고 확률을 만들어, 트리들이 예측한 확률을 평균내어 가장 높은 확률을 가진 클래스를 예측값으로 정함\n",
    "- 회귀를 할 때는 모든 트리에서의 예측을 평균함\n",
    "- 기본 설정으로 좋은 결과를 만들어줄 때가 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fa9a1-2a54-47bd-99b8-5276a2fd725b",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:purple\">랜덤 포레스트 분석</span>** \n",
    "- 랜덤 포레스트 안에 만들어진 트리는 estimators_속성에 저장됨\n",
    "- 특성 중요도는 하나의 트리에서 제공하는 것보다 신뢰할 만함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945a922-8242-47cd-a292-b94925e312ee",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:purple\">장단점과 매개변수</span>** \n",
    "- 성능이 매우 우수하고, 매개변수 튜닝을 많이 하지 않아도 잘 작동하며, 데이터의 스케일을 맞출 필요가 없음\n",
    "- 단일 트리보다는 시각적으로 과정을 보여주기는 어려움\n",
    "- 대량의 데이터셋에서는 다소 시간이 걸릴 수 있지만, CPU코어가 많다면 병렬처리할 수 있음 -> 대량의 데이터셋에도 잘 작동함\n",
    "- 트리가 많을수록 random_state 값의 변화에 따른 변동이 적음. 그렇지 않다면 전혀 다른 모델이 만들어짐\n",
    "- 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않음 -> 이런 데이터는 선형 모델이 더 적합함\n",
    "- 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느림\n",
    "- 중요한 매개변수는 n_estimators와 max_features(일반적으로 기본값을 쓰는 것이 좋은 방법임)이고, 부수적으로 사전 가지치기 옵션(훈련 메모리와 시간을 줄일 수 있음)이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b8214-36ec-4b9a-bd65-04d865c2ea91",
   "metadata": {},
   "source": [
    "### **그레이디언트 부스팅 회귀 트리**\n",
    "- 여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또 다른 앙상블 방법\n",
    "- 회귀와 분류 모두에 사용할 수 있음\n",
    "- 랜덤 포레스트와 달리 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듬\n",
    "- 기본적으로는 무작위성이 없으나 subsample 매개변수로 훈련 샘플의 비율을 지정할 수 있고, max_features 매개변수로 특성의 개수를 지정할 수 있음\n",
    "- 강력한 사전 가지치기가 사용됨\n",
    "- 보통 1~5 정도의 깊지 않은 트리를 사용하므로 메모리를 적게 사용하고 예측이 빠름(얕은 트르 같은 간단한 모델을 많이 연결하는 것)\n",
    "- 각각의 트리는 데이터의 일부에 대해서만 예측을 잘 수행할 수 있어서, 트리가 많이 추가될수록 성능이 좋아짐\n",
    "- 랜덤 포레스트보다는 매개변수 설정에 조금 더 민감하지만, 잘 조정하면 더 높은 정확도를 제공해줌\n",
    "- learning_rate 매개변수가 중요함 -> 이전 트리의 오차를 얼마나 강하게 보정할지 제어함. 학습률이 높을수록 보정이 강해지며 복잡한 모델을 만듬\n",
    "- n_estimators 값을 키우면 트리가 많이 추가되어 모델의 복잡도가 커지고, 훈련 세트에서의 실수를 바로잡을 기회가 더 많아짐\n",
    "- 보통 먼저 더 안정적인 랜덤 포레스트를 적용하고, 예측 시간이 중요하거나 머신러닝 모델에서 마지막 성능까지 쥐어짜야할 때 그레이디언트 부스팅을 사용하면 도움이 됨\n",
    "- 대규모 데이터셋에서는 xgboost 패키지가 더 빠르게 작동하고 튜닝하기도 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17698514-57f9-436d-8475-fd78984c7902",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:purple\">장단점과 매개변수</span>** \n",
    "- 매개변수를 잘 조정해야 한다는 것과 훈련 시간이 길다는 단점이 있음(순차적으로 트리를 학습하기 때문에 n_jobs 매개변수가 없음)\n",
    "- 데이터의 스케일을 조정하지 않아도 됨\n",
    "- 희소한 고차원 데이터에는 잘 작동하지 않음\n",
    "- n_estimators와 learning_rate 매개변수가 중요함\n",
    "- 랜덤 포레스트와 달리 n_estimators가 클수록 모델이 복잡해지고 과대적합될 가능성이 있음\n",
    "- 일반적으로 가용한 시간과 메모리 한도에서 n_estimators를 맞추고 나서 적절한 learning_rate를 찾음\n",
    "- 조기 종료를 위한 n_iter_no_change와 validation_fraction이 추가됨. validation_fraction 비율만큼 검증 데이터로 사용하여 n_iter_no_change 반복 동안 검증 점수가 향상되지 않으면 훈련이 종료됨\n",
    "- 각 트리의 복잡도를 낮추는 max_depth와 max_leaf_nodes 매개변수가 있음. 통상적으로 max_depth를 매우 작게 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038ce38-7d87-4dda-8d6e-bd7eab88c873",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">그 외의 다른 앙상블</span>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "edc88cc2-ae0d-46bd-8193-3b89e5cabc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, load_breast_cancer\n",
    "\n",
    "Xm,ym=make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "Xm_train,Xm_test,ym_train,ym_test=train_test_split(\n",
    "    Xm,ym,stratify=ym, random_state=42)\n",
    "\n",
    "cancer=load_breast_cancer()\n",
    "Xc_train,Xc_test,yc_train,yc_test=train_test_split(\n",
    "    cancer.data,cancer.target,stratify=cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6d62d-c10e-408b-9ea6-b8a938d9e0ac",
   "metadata": {},
   "source": [
    "### **배깅**\n",
    "- Bootstrap aggregating의 줄임말 -> 중복을 허용한 랜덤 샘플링으로 만든 훈련 세트를 사용하여 분류기를 각각 다르게 학습시킴\n",
    "- 분류기가 predict_proba() 메서드를 지원하는 경우에는 확률값을 평균하여 예측을 수행하고, 그렇지 않은 분류기를 사용할 때는 빈도가 높은 클래스 레이블이 예측 결과가 됨\n",
    "- 램덤 포레스트는 DecisionTreeClassifier(splitter=\"best\")로 고정되어 있지만, bagging을 통해 splitter=\"random\"을 할 수 있음 -> 노드를 분할할 때 무작위로 특성과 분할 지점을 선택함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24877be-b26b-4832-bfa9-d0550924534b",
   "metadata": {},
   "source": [
    "### **엑스트라 트리**\n",
    "- 랜덤 포레스트와 비슷하지만 후보 특성을 무작위로 분할한 다음 최적의 분할을 찾음\n",
    "- DecisionTreeClassifier(spliter=\"random\")을 사용하고 부트스트랩 샘플링은 기본적으로 적용하지 않음\n",
    "- 무작위성을 증가시키면 일반적으로 모델의 편향이 늘어나지만 분산이 감소함\n",
    "- 예측 방식은 랜덤 포레스트와 동일함\n",
    "- 개별 트리는 후보 노드를 랜덤하게 분할한 다음, 최선의 분할을 찾기 때문에 결정 경계가 복잡함\n",
    "- 앙상블한 엑스트라 트리의 결정 경계는 비교적 안정적임\n",
    "- 랜덤 포레스트보다 계산 비용이 적지만, 일반화 성능을 높이려면 많은 트리를 만들어야 하기 때문에 랜덤 포레스트가 더 선호됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860ba8d-2da5-407f-888c-8531f2dccea0",
   "metadata": {},
   "source": [
    "### **에이다부스트**\n",
    "- Adaptive Boosting의 줄임말임\n",
    "- 그레이디언트 부스팅처럼 약한 학습기를 사용함 <-> 그레이디언트 부스팅과 달리 이전의 모델이 잘못 분류한 샘플에 가중치를 높여서 다음 모델을 훈련시킴\n",
    "- 훈련된 각 모델은 성능에 따라 가중치가 부여됨\n",
    "- 예측을 만들 때는 모델이 예측한 레이블을 기준으로 모델의 가중치를 합산하여 가장 높은 값을 가진 레이블을 선택함\n",
    "- 순차적으로 학습하기 때문에 n_jobs 매개변수가 없음\n",
    "- classifier은 decisiontreeclassifier의 max_depth가 1이고, regressor은 decisionregressor의 max_depth가 3으로 고정되어 있지만, base_estimator에서 모델을 직접 설정할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2913c93-770c-441f-b9b8-69a1feee6cf6",
   "metadata": {},
   "source": [
    "### **히스토그램 기반 부스팅**\n",
    "- 입력 특성을 256개의 구간으로 나누기 때문에 노드를 분할할 때 최적의 분할을 빠르게 찾을 수 있음\n",
    "- 256개 구간 중 하나를 누락된 값을 위해서 사용하기 때문에 누락된 값을 위해 전처리할 필요가 없음\n",
    "- 일반적으로 대규모 셋에서 그레이디언트 부스팅보다 훨씬 빠름\n",
    "- 기본 매개변수에서도 비교적 좋은 성능을 보임 -> max_iter 매개변수로 트리의 개수를 증가시킬 수 있음\n",
    "- 사이킷런에서는 특성 중요도를 제공하지 않음 -> permutation_importance 함수를 사용하여 특성 중요도를 계산할 수 있음. 특성 값을 차례대로 섞은 후 모델의 성능을 평가하여 어떤 특성이 중요한 역할을 하는지 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98606c3b-806c-4513-ad52-0825f8c4bc01",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">커널 서포트 벡터 머신</span>** \n",
    "- 입력 데이터에서 단순한 초평면으로 정의되지 않는 더 복잡한 모델을 만들 수 있도록 확장한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f90f4-c318-4fe9-aff7-ae119c04122f",
   "metadata": {},
   "source": [
    "### **선형 모델과 비선형 모델**\n",
    "- 직선과 초평면은 유연하지 못하여 저차원 데이터셋에서는 선형 모델이 제한적임\n",
    "- 유연하게 만들기 위해서 특성끼리 곱하거나 특성을 거듭제곱하는 식으로 특성을 추가하는 방법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934822f-b606-4e28-b2a8-ab3afc473a84",
   "metadata": {},
   "source": [
    "### **커널 기법**\n",
    "- 많은 경우에 어떤 특성을 추가해야 할지 모르고, 특성을 많이 추가할수록 연산 비용이 커짐\n",
    "- 수학적 방법을 이용해서 새로운 특성을 많이 만들지 않고서도 고차원에서 분류기를 학습시킬 수 있음 -> 커널 기법(데이터를 확장하지 않고, 확장된 특성에 대한 데이터 포인트들의 거리를 계산함)\n",
    "- 데이터를 고차원 공간에 매핑하는 방법은 두 가지임\n",
    "   - 다항식 커널: 원래 특성의 가능한 조합을 지정된 차수까지 모두 계산함\n",
    "   - RBF(가우시안 커널): 차원이 무한한 특성 공간에 매핑하는 것으로, 특성의 중요도는 고차원이 될수록 줄어듬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6f9e3-1f1c-4e74-98b1-fcea18f90031",
   "metadata": {},
   "source": [
    "### **SVM 이해하기**\n",
    "- 각 훈련 데이터 포인트가 두 클래스 사이의 결정 경계를 구분하는 데 얼마나 중요한지를 배움\n",
    "- 일반적으로 훈련 데이터의 일부(두 클래스 사이의 경계에 위치한 데이터 포인트들)만 결정 경계를 만드는 데 영향을 줌 -> 이 데이터 포인트를 서포트 벡터라고 함\n",
    "- 새로운 데이터 포인트에 대해 예측하려면 각 서포트 벡터와의 거리를 측정함\n",
    "- 서포트 벡터의 중요도는 훈련 과정에서 학습하며 SVC 객체의 dual_coef_ 속성에 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241840b-0271-479e-83e3-7d781869e8ca",
   "metadata": {},
   "source": [
    "### **SVM 매개변수 튜닝**\n",
    "- gamma 매개변수는 가우시안 커널의 폭을 제어하는 매개변수임. 가우시안 커널 폭의 역수 -> 하나의 훈련 샘플이 미치는 영향의 범위를 결정함. 작은 값일수록 넓은 영역을 뜻함. 가우시안 커널의 반경이 클수록 훈련 샘플의 영향 범위도 커짐. gammma 값이 작을수록 데이터 포인트의 영향 범위가 커(모델의 복잡도를 낮춤)\n",
    "- C 매개변수는 선형 모델에서 사용한 것과 비슷한 규제 매개변수임. 각 포인트의 중요도를 제한함\n",
    "- 매개변수 설정과 데이터 스케일에 매우 민감함. 특히 입력 특성의 범위가 비슷해야 함\n",
    "- C와 gamma모두 클수록 더 복잡한 모델을 만듬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01276831-a180-425c-bb09-4d7dc7d60eaf",
   "metadata": {},
   "source": [
    "### **SVM을 위한 데이터 전처리**\n",
    "- 특성 값의 범위가 비슷해지도록 조정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71a5c9-2802-434a-82fc-0ca79eb24700",
   "metadata": {},
   "source": [
    "### **장단점과 매개변수**\n",
    "- 데이터의 특성이 몇개 안 되더라고 복잡한 결정 경계를 만들 수 있음\n",
    "- 특성이 적을 때와 많을 때 모두 잘 작동하지만, 샘플 수가 많을 때는 잘 맞지 않음(메모리가 매우 많이 듬)\n",
    "- 데이터 전처리와 매개변수 설정에 신경을 많이 써야 함 -> 요즘에는 트리 기반 모델(랜덤 포레스트, 그레이디언트 부스팅 등)을 많이 사용함\n",
    "- 모델을 분석하기 어렵고, 비전문가에게 설명하기가 어려움\n",
    "- 모든 특성이 비슷한 단위이고 스케일이 비슷하다면 SVM을 시도해 볼만함\n",
    "- C, 어떤 커널을 사용할지, 각 커널에 따른 매개변수가 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f91de6-3366-4577-94e7-177c452d53da",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">신경망(딥러닝)</span>** \n",
    "- 이 책에서는 다층 퍼셉트론(MLP)을 다룸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7db9e-45ed-48fb-8366-3bae4e629073",
   "metadata": {},
   "source": [
    "### **신경망 모델**\n",
    "- MLP: 여러 단계를 거쳐 결정을 만들어내는 선형 모델의 일반화된 모습. 입력, 출력, 은닉층의 유닛들이 모두 연결되어 있어서 완전 연결 신경망이라고도 함. feed-forward 신경망이라고도 함\n",
    "- 중간 단계를 구성하는 은닉 유닛을 계산하고, 이를 이용하여 최종 결과를 산출하기 위해 다시 가중치 합을 계산함\n",
    "- 많은 계수(가중치)를 학습해야 함. 이 계수는 각 입력과 은닉층의 은닉 유닛 사이, 각 은닉 유닛과 출력 사이마다 있음\n",
    "- 각 은닉 유닛의 가중치 합을 계산한 후 그 결과에 비선형 함수인 렐루, 하이퍼볼릭 탄젠트, 로지스틱 함수를 적용함 -> 활성화 함수라고 함\n",
    "- 보통 분류의 경우에는 마지막 출력층에 시그모이드(이진 분류)나 소프트맥스(다중 분류) 함수를 적용하여 최종 출력을 계산함\n",
    "- 은닉층의 유닛 개수 매개변수를 정하는 것이 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6c6ae-3b0a-47b3-b569-0718efa28698",
   "metadata": {},
   "source": [
    "### **신경망 튜닝**\n",
    "- 은닉층의 수, 은닉층의 유닛 개수, 규제, 드롭 아웃 등 신경망의 복잡도를 제어하는 방법이 많음\n",
    "- 모든 입력 특성을 표준화시켜주는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422da58-a228-4414-9c60-8295798b6627",
   "metadata": {},
   "source": [
    "### **장단점과 매개변수**\n",
    "- 대량의 데이터에 내재된 정보를 잡아내고 매우 복잡한 모델을 만들 수 있음\n",
    "- 충분한 연산 시간과 매개변수를 세심하게 조정하면 종종 다른 머신러닝 알고리즘을 뛰어넘는 성능을 냄\n",
    "- 신경망은 크고 강력한 모델일수록 학습이 오래 걸림\n",
    "- 데이터 전처리에 주의해야 함\n",
    "- 모든 특성이 같은 의미를 가진 동질의 데이터에서 잘 작동함 <-> 다른 종류의 특성을 가진 데이터라면 트리 기반 모델이 더 잘 작동할 수 있음\n",
    "- 매개변수 튜닝이 매우 세심함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935744e1-4673-4335-9329-8224a2e2009d",
   "metadata": {},
   "source": [
    "### **신경망의 복잡도 추정**\n",
    "- 가장 중요한 매개변수는 은닉층의 개수와 각 은닉층의 유닛 수임 -> 한 개 또는 두 개의 은닉층으로 시작해서 늘려가기\n",
    "- 각 은닉층의 유닛 수는 보통 입력 특성 수와 비슷하게 설정하지만, 수천 개를 넘는 일은 거의 없음\n",
    "- 신경망의 모델 복잡도를 계산하는 방법은 학습된 가중치를 계산하는 방법이 있음 -> 특성 100개와 은닉 유닛 100개(은닉층 1개)를 가진 이진 분류라면 10201개의 가중치가 있음\n",
    "- 신경망의 매개변수를 조정하는 일반적인 방법은 충분히 과대적합되어 문제를 해결할 만한 큰 모델을 만들고, 신경 구조를 줄이거나 규제 강화를 위해 alpha값을 증가시켜 일반화 성능을 향상시킴\n",
    "- solver 매개변수는 대표적으로 두 가지가 있음\n",
    "   - adam: 대부분의 경우 잘 작동하지만 데이터의 스케일에 조금 민감함 -> 표준화 권장\n",
    "   - lbfgs: 안정적이지만 규모가 큰 모델이나 대량의 데이터셋에서는 시간이 오래 걸림\n",
    "   - sgd: 고급 옵션임. 다른 여러 매개변수와 함께 튜닝하여 최선의 결과를 만들 수 있음 -> 모멘텀 매개변수를 잘 설정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6b9fb-4dd5-406c-aa82-9da0c4a362d9",
   "metadata": {},
   "source": [
    "# **<span style=\"color:blue\">분류 예측의 불확실성 추정</span>** \n",
    "---\n",
    "- 사이킷런에는 불확실성을 추정할 수 있는 함수가 두 개 있음\n",
    "   - decision_funciton\n",
    "   - predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1690ae-bd58-4ea0-8b59-0b812a57c5ed",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">결정 함수</span>** \n",
    "- 데이터 포인트가 양성 클래스인 클래스 1에 속한다고 믿는 정도\n",
    "- 양수는 양성 클래스, 음수는 음성 클래스를 의미함\n",
    "- decision_function 값의 범위는 데이터와 모델 파라미터에 따라 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e68709-fcc3-4e5a-a1a6-eaa686458902",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">예측 확률</span>** \n",
    "- 각 클래스에 대한 확률\n",
    "- 과대적합된 모델은 잘못된 예측이더라도 예측의 확신의 강한 편임\n",
    "- 일반적으로 복잡도가 낮은 모델은 예측에 불확실성이 더 많음\n",
    "- 보정: 불확실성과 모델의 정확도가 동등함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29312470-468d-4b4c-8d59-cbfefab482db",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">다중 분류에서의 확실성</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f544b33-004b-49c5-8747-0d42f832a104",
   "metadata": {},
   "source": [
    "## **<span style=\"color:green\">요약 및 정리</span>** \n",
    "- 최근접 이웃 -> 작은 데이터셋일 경우, 기본 모델로서 좋고 설명하기 쉬움\n",
    "- 선형 모델 -> 첫 번째로 시도할 알고리즘. 대용량 데이터셋 가능. 고차원 데이터에 가능\n",
    "- 나이브 베이즈 -> 분류만 가능. 선형 모델보다 훨씬 빠름. 대용량 데이터셋과 고차원 데이터에 가능. 선형모델보다 덜 정확함\n",
    "- 결정 트리 -> 매우 빠름. 데이터 스케일 조정 필요 없음. 시각화하기 좋고 설명하기 쉬움\n",
    "- 랜덤 포레스트 -> 결정 트리 하나보다 좋은 성능을 냄. 매우 안정적이고 강력함. 데이터 스케일 조정 필요 없음. 고차원 희소 데이터에는 잘 안 맞음\n",
    "- 그레이디언트 부스팅 결정 트리 -> 랜덤 포레스트보다 조금 더 성능이 좋음. 랜덤 포레스트보다 학습은 느리나, 예측은 빠르고 메모리를 조금 사용. 랜덤 포레스트보다 매개변수 튜닝이 많이 필요함\n",
    "- 서포트 벡터 머신 -> 비슷한 의미의 특성으로 이뤄진 중간 규모 데이터셋에 잘 맞음. 데이터 스케일 조정 필요. 매개변수에 민감\n",
    "- 신경망 -> 특별히 대용량 데이터셋에서 매우 복잡한 모델을 만들 수 있음. 매개변수 선택과 데이터 스케일에 민감. 큰 모델은 학습이 오래 걸림"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
